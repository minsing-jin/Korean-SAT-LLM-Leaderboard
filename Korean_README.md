![image](https://github.com/user-attachments/assets/013fead4-495b-4cd8-9019-f64fe8447a22)

# ğŸ† ìˆ˜ëŠ¥ êµ­ì–´ LLM ë¦¬ë”ë³´ë“œ

--------------
## ğŸ—‚ï¸ Index
- [ğŸ’¯ ë¦¬ë”ë³´ë“œ ìˆœìœ„](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-%EB%A6%AC%EB%8D%94%EB%B3%B4%EB%93%9C-%EC%88%9C%EC%9C%84)
- [ğŸ¯ ìˆ˜ëŠ¥ êµ­ì–´ LLM Leaderboardë€?](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-%EC%88%98%EB%8A%A5-%EA%B5%AD%EC%96%B4-llm-leaderboard%EB%9E%80)
- [ğŸ¥‡ Submit ë°©ì‹](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-submit-%EB%B0%A9%EC%8B%9D)
- [ğŸª‘ About benchmark Datadataset](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-benchmark-%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B)
- [â™¾ï¸ Metric](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#%EF%B8%8F-metric)
- [ğŸ“— ì°¸ê³ í•´ë³¼ë§Œí•œ Reference](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#%EF%B8%8F-metric)
- [ğŸ“° Notice](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-notice)
- [ğŸ“¬ ë¬¸ì˜í•˜ê¸°](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/Korean_README.md#-%EB%AC%B8%EC%9D%98%ED%95%98%EA%B8%B0)

### [2023 ìˆ˜ëŠ¥ ìƒ˜í”Œ ë°ì´í„°ì…‹ ëª¨ë¸ í…ŒìŠ¤íŠ¸ í•˜ëŠ” ë°©ë²•](https://github.com/minsing-jin/Korean-SAT-LLM-Leaderboard/blob/main/korean_sat_mini_test/manual/Kor_manual.md)

------------
ìˆ˜ëŠ¥ êµ­ì–´ LLM ë²¤ì¹˜ë§ˆí¬ ë¦¬ë”ë³´ë“œë¡œ Human performanceì™€ LLMì˜ Performanceë¥¼ ë¹„êµí•´ë³´ì„¸ìš”!

ì—¬ëŸ¬ë¶„ì´ ê°œë°œí•œ í•œêµ­ì–´ LLM íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ì¸ê°„ì˜ performanceë¥¼ ë„˜ì–´ ìƒˆë¡œìš´ ì—­ì‚¬ë¥¼ ì“¸ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆê¹Œ? ì§€ê¸ˆ ë°”ë¡œ ë²¤ì¹˜ë§ˆí¬ì—ì„œ ëª¨ë¸ì˜ í•œê³„ë¥¼ ì‹œí—˜í•˜ê³ , ìˆ˜ëŠ¥ êµ­ì–´ì—ì„œ SOTA(State Of The
Art) ìë¦¬ë¥¼ ì°¨ì§€í•˜ì„¸ìš”!ğŸ‘‘


## ğŸ’¯ ë¦¬ë”ë³´ë“œ ìˆœìœ„

| Leaderboard Rank |             Model Name             | Submitter Name | Avg. std Score | Avg. Grade | 2024 SAT | 2023 SAT | 2022 SAT | 2021 SAT | 2020 SAT | 2019 SAT | 2018 SAT | 2017 SAT | 2016 SAT | 2015 SAT | URL                                                                                                                                    |
|:----------------:|:----------------------------------:|:--------------:|:--------------:|:----------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:--------:|:---------------------------------------------------------------------------------------------------------------------------------------|
|    ğŸ¥‡ **1st**    |         gpt-4o-2024-08-06          |     OpenAI     |     114.9      |    3.6     |  65 (4)  |  81 (4)  |  70 (4)  |  69 (4)  |  76 (4)  |  74 (3)  |  77 (4)  |  86 (2)  |  84 (3)  |  77 (4)  | [Link](https://openai.com/)                                                                                                            |
|    ğŸ¥ˆ **2nd**    | Meta-Llama-3.1-405B-Instruct-Turbo |   meta-llama   |     113.8      |    3.8     |  77 (3)  |  87 (3)  |  69 (4)  |  70 (4)  |  65 (5)  |  68 (4)  |  78 (4)  |  80 (3)  |  87 (3)  |  68 (5)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct)                                                                      |
|    ğŸ¥‰ **3rd**    |     Qwen2.5-72B-Instruct-Turbo     |      Qwen      |     105.8      |    4.6     |  61 (5)  |  78 (4)  |  52 (6)  |  60 (5)  |  60 (5)  |  64 (4)  |  74 (4)  |  70 (5)  |  74 (4)  |  79 (4)  | [Link](https://huggingface.co/Qwen/Qwen2.5-72B-Instruct)                                                                               |
|       4th        | Meta-Llama-3.1-70B-Instruct-Turbo  |   meta-llama   |     103.7      |    4.8     |  50 (6)  |  72 (5)  |  73 (3)  |  61 (5)  |  79 (3)  |  51 (5)  |  58 (6)  |  66 (5)  |  71 (5)  |  70 (5)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-70B-Instruct)                                                                       |
|       5th        |         Qwen2-72B-Instruct         |      Qwen      |       98       |    5.2     |  53 (5)  |  57 (6)  |  59 (5)  |  45 (6)  |  57 (5)  |  56 (5)  |  76 (4)  |  69 (5)  |  58 (6)  |  63 (5)  | [Link](https://huggingface.co/Qwen)                                                                                                    |
|       6th        |       gpt-4o-mini-2024-07-18       |     OpenAI     |      93.9      |    5.6     |  57 (5)  |  53 (6)  |  50 (6)  |  55 (5)  |  50 (6)  |  46 (6)  |  62 (5)  |  58 (6)  |  64 (5)  |  57 (6)  | [Link](https://openai.com/)                                                                                                            |
|       7th        |           gemma-2-27b-it           |     Google     |       91       |    5.9     |  51 (6)  |  54 (6)  |  51 (6)  |  51 (6)  |  50 (6)  |  37 (7)  |  50 (6)  |  71 (4)  |  54 (6)  |  56 (6)  | [Link](https://huggingface.co/google/gemma-2-27b-it)                                                                                   |
|       8th        |           solar-mini-ja            |    Upstage     |      85.9      |    6.2     |  46 (6)  |  58 (6)  |  43 (6)  |  41 (7)  |  46 (6)  |  51 (5)  |  49 (6)  |  48 (7)  |  40 (7)  |  52 (6)  | [Link](https://ko.upstage.ai/feed/company/event-recap-exploring-japan-ai-scene-with-upstage-solar-mini-ja)                             |
|       9th        |             solar-mini             |    Upstage     |      85.5      |    6.4     |  33 (7)  |  57 (6)  |  48 (6)  |  42 (7)  |  46 (6)  |  50 (6)  |  43 (7)  |  55 (6)  |  42 (7)  |  56 (6)  | [Link](https://www.upstage.ai/feed/product/solarmini-performance-report)                                                               |
|       10th       |    Mixtral-8x22B-Instruct-v0.1     |   MistralAI    |      83.4      |    6.6     |  40 (7)  |  44 (7)  |  47 (6)  |  31 (8)  |  38 (7)  |  35 (7)  |  65 (5)  |  57 (6)  |  50 (6)  |  44 (7)  | [Link](https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1)                                                                   |
|       11th       |          WizardLM-2-8x22B          |   Microsoft    |      83.3      |    6.6     |  37 (7)  |  56 (6)  |  47 (6)  |  30 (8)  |  52 (6)  |  29 (8)  |  51 (6)  |  47 (7)  |  51 (6)  |  53 (6)  | [Link](https://www.microsoft.com/en-us/research/publication/wizardlm-empowering-large-language-models-to-follow-complex-instructions/) |
|       12th       |     Qwen2.5-7B-Instruct-Turbo      |      Qwen      |      80.3      |    6.8     |  40 (7)  |  40 (7)  |  39 (7)  |  35 (7)  |  35 (7)  |  35 (7)  |  58 (6)  |  53 (6)  |  44 (7)  |  42 (7)  | [Link](https://huggingface.co/Qwen/Qwen2.5-72B)                                                                                        |
|       13th       |  Meta-Llama-3.1-8B-Instruct-Turbo  |   meta-llama   |      74.7      |    7.1     |  46 (6)  |  31 (8)  |  36 (7)  |  34 (7)  |  36 (7)  |  24 (8)  |  38 (7)  |  38 (7)  |  37 (7)  |  45 (7)  | [Link](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)                                                                        |
|       14th       |         gpt-3.5-turbo-0125         |     OpenAI     |      68.7      |    7.7     |  29 (8)  |  39 (7)  |  26 (8)  |  17 (9)  |  36 (7)  |  24 (8)  |  38 (7)  |  25 (8)  |  45 (7)  |  27 (8)  | [Link](https://openai.com/)                                                                                                            |
|       15th       |     Mixtral-8x7B-Instruct-v0.1     |   MistralAI    |      63.4      |    8.3     |  19 (9)  |  25 (8)  |  40 (7)  |  20 (9)  |  27 (8)  |  19 (9)  |  37 (7)  |  16 (9)  |  30 (8)  |  19 (9)  | [Link](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)                                                                    |
|       16th       |           gemma-2-9b-it            |     Google     |      61.2      |    8.4     |  24 (8)  |  20 (9)  |  16 (9)  |  22 (9)  |  17 (9)  |  29 (8)  |  24 (8)  |  25 (8)  |  25 (8)  |  29 (8)  | [Link](https://huggingface.co/google/gemma-2-9b-it)                                                                                    |
|       17th       |    Llama-3.2-3B-Instruct-Turbo     |   meta-llama   |      60.6      |    8.7     |  28 (8)  |  18 (9)  |  27 (8)  |  23 (9)  |  16 (9)  |  17 (9)  |  21 (9)  |  29 (8)  |  22 (9)  |  23 (9)  | [Link](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)                                                                        |
|       18th       |      Mistral-7B-Instruct-v0.3      |   MistralAI    |      57.2      |    8.9     |  17 (9)  |  11 (9)  |  22 (9)  |  12 (9)  |  18 (9)  |  21 (9)  |  19 (9)  |  27 (8)  |  23 (9)  |  21 (9)  | [Link](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3)                                                                      |


- **Rank ê¸°ì¤€**: 10ê°œë…„ ìˆ˜ëŠ¥ í‘œì¤€ì ìˆ˜ë“¤ì˜ í‰ê·   <í‘œì¤€ì ìˆ˜ë¥¼ í†µí•´ì„œ ê° ë…„ë„ë³„ ì‹œí—˜ì˜ ë‚œì´ë„ë¥¼ ì ìˆ˜ì—ì„œ ë°˜ì˜í•©ë‹ˆë‹¤.>
- **Avg. std Score:** í‘œì¤€ì ìˆ˜ í‰ê· 
- **Avg. Grade:** ë“±ê¸‰ í‰ê· 
- **ë…„ë„ë³„ ìˆ˜ëŠ¥ ì ìˆ˜ í‘œê¸°**: ì›ì ìˆ˜(ë“±ê¸‰)

[ì ìˆ˜ì— ëŒ€í•œ ì„¤ëª… ë°”ë¡œê°€ê¸°](https://github.com/minsing-jin/KO-SAT_Slayer_Champions_League/blob/main/Korean_README.md#%EF%B8%8F-metric)

i.e)

- GPU ìì› Donationí•´ì£¼ì‹ ë‹¤ë©´ í‰ê°€ì— í° ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤!
- ì ê·¹ì ì¸ í”¼ë“œë°± í™˜ì˜í•©ë‹ˆë‹¤! ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”!

### ğŸ“— Notes. 24 ìˆ˜ëŠ¥ (1ê°œë…„) ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ê²°ê³¼

- **o1-preview**: 88ì  (1ë“±ê¸‰, ìƒìœ„ 4%)
- **o1-mini** : 60ì  (5ë“±ê¸‰)

**i.e)** gpt o1 ëª¨ë¸ì€ o1 ì •ì‹ë²„ì „ì´ ì¶œì‹œí• ë•Œ ë²¤ì¹˜ë§ˆí¬ ì—…ë°ì´íŠ¸ ì˜ˆì •ì…ë‹ˆë‹¤!


---

## ğŸ¯ ìˆ˜ëŠ¥ êµ­ì–´ LLM Leaderboardë€?

ìˆ˜ëŠ¥ êµ­ì–´ LLM LeaderboardëŠ” í•œêµ­êµìœ¡ê³¼ì •í‰ê°€ì›(KICE)ì´ ê°œë°œí•œ ëŒ€í•™ìˆ˜í•™ëŠ¥ë ¥ì‹œí—˜(ìˆ˜ëŠ¥) êµ­ì–´ ê³¼ëª©ì˜ 10ê°œë…„ ì‹œí—˜ë¬¸ì œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë²¤ì¹˜ë§ˆí¬ ë¦¬ë”ë³´ë“œì…ë‹ˆë‹¤.

ë§¤ë…„ ì—„ì„ ëœ ìˆ˜ëŠ¥ êµ­ì–´ ì‹œí—˜ ë¬¸ì œë¥¼ í†µí•´ ì—¬ëŸ¬ë¶„ì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‰ê°€ ë°©ì‹ì€ ì‹¤ì œ ìˆ˜ëŠ¥ê³¼ ë™ì¼í•˜ê²Œ í‘œì¤€ì ìˆ˜ì™€ ë“±ê¸‰ ì²´ê³„ë¥¼ ì‚¬ìš©í•˜ì—¬, Human performanceì™€ LLMì˜
performanceì„ ì§ì ‘ ë¹„êµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.

## ğŸ… Submit ë°©ì‹

- ë¦¬ë”ë³´ë“œ ê³µê°œë¥¼ ì›í•˜ì§€ ì•Šê³ , privateí•˜ê²Œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì•Œê³  ì‹¶ë‹¤ë©´ í•˜ê³  ì‹¶ì€ ë§ íŒŒíŠ¸ì— ë‚¨ê²¨ì£¼ì„¸ìš”!
- â­ï¸ ìˆ˜ëŠ¥ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ëª¨ë¸ì˜ context lengthëŠ” ìµœì†Œ 8K ì´ìƒì´ì–´ì•¼í•©ë‹ˆë‹¤!

1. **ëª¨ë¸ submission**:
    - **[ì„¤ë¬¸ Formìœ¼ë¡œ ì œì¶œ](https://moaform.com/q/QP0AV0)**: ì„¤ë¬¸ ì‘ë‹µì— ë§ì¶° ì‘ì„±í•´ì£¼ì„¸ìš”!
        - ë§í¬: https://moaform.com/q/QP0AV0
    - **ì´ë©”ì¼ë¡œ ì œì¶œ**: huggingFaceì— ê²Œì‹œëœ ìì‹ ì˜ finetuning ëª¨ë¸ì˜ Urlê³¼ ë‹‰ë„¤ì„ì„ ì „ì†¡í•´ì£¼ì„¸ìš”!
        - ì œì¶œ ë©”ì¼: developerminsing@gmail.com
    - **issueë¡œ ì œì¶œ**: Githubì˜ ì´ìŠˆì—ì„œ ìì‹ ì˜ finetuning ëª¨ë¸ì˜ Urlê³¼ ë‹‰ë„¤ì„ì„ ê²Œì‹œí•´ì£¼ì„¸ìš”!
    ```markdown
   <ì´ë©”ì¼ ì œì¶œ, ì´ìŠˆ ì œì¶œì‹œ Form example>
    ì œì¶œì ì´ë¦„: ê°ìŠ¤íŠ¸
    HuggingFace ì œì¶œ URL: https://huggingface.co/ê°ìŠ¤íŠ¸ëª¨ë¸ì§œìŠ¤
    í•˜ê³  ì‹¶ì€ë§: ì—´ì‹¬íˆ í•˜ì‹œì–ì•„
    ```

2. **ë¦¬ë”ë³´ë“œ í™•ì¸**: githubì™€ huggingFaceì—ì„œ ìì‹ ì˜ ìˆœìœ„ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
3. **ìˆœìœ„ ìƒìŠ¹ ë„ì „**: ìì‹ ì˜ ìˆœìœ„ë¥¼ ì˜¬ë ¤ **Slayer Champion** íƒ€ì´í‹€ì„ íšë“í•˜ì„¸ìš”.

**Notice:** ëª¨ë¸ ì œì¶œí›„ ê°€ìš©í•œ GPU ë¦¬ì†ŒìŠ¤ì™€, ì œì¶œëŸ‰ì— ë”°ë¼ 1~3ì£¼ì¼ì˜ ì‹œê°„ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸª‘ Benchmark ë°ì´í„°ì…‹

- ë³¸ ëŒ€íšŒì—ì„œëŠ” 2015ë…„ë¶€í„° 2024ë…„ê¹Œì§€ì˜ 10ê°œë…„ ìˆ˜ëŠ¥ êµ­ì–´ ë¬¸ì œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
- 2022ë…„ë„ë¶€í„° ì‹œí–‰ëœ ì„ íƒê³¼ëª©ì— ëŒ€í•´ì„œëŠ” í™”ë²•ê³¼ ì‘ë¬¸ ê³¼ëª© ì„ íƒê³¼ëª©ìœ¼ë¡œ í•˜ì—¬ benchmarkë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.
- Benchmark ë°ì´í„°ì…‹ì˜ ì£¼ìš” í‰ê°€ ëª©ë¡ì€ ì–¸ì–´ ì´í•´ë ¥, í•µì‹¬ ë‚´ìš© íŒŒì•… ëŠ¥ë ¥, ë…¼ë¦¬ì  ì‚¬ê³ ë ¥, ë¹„íŒì  ì‚¬ê³ ë ¥, ì°½ì˜ì  ì‚¬ê³ ë ¥, ë©€í‹°ë¯¸ë””ì–´ í•´ì„ë ¥ì„ í‰ê°€í•©ë‹ˆë‹¤.
  <ì¶œì²˜: 2024 KICE ìˆ˜ëŠ¥ êµ­ì–´ í‰ê°€ ëª©ë¡>

## ğŸ“‘ Benchmark ë°ì´í„°ì…‹ Category

### (1) ğŸ“š ë…ì„œ

ì¶œì œ ë‚´ìš©: ì¸ë¬¸, ì‚¬íšŒ, ê³¼í•™, ê¸°ìˆ , ì˜ˆìˆ  ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì˜ ì§€ë¬¸ì´ ì¶œì œë©ë‹ˆë‹¤.

- **ì¸ë¬¸ ì§€ë¬¸**: ì² í•™, ë…¼ì¦, ì—­ì‚¬ ë“±ì„ ë‹¤ë£¹ë‹ˆë‹¤.
- **ì‚¬íšŒ ì§€ë¬¸**: ê²½ì œ, ì •ì¹˜, ë²•, ë¬¸í™” ë“±ì„ ë‹¤ë£¹ë‹ˆë‹¤.
- **ê³¼í•™ ì§€ë¬¸**: ë¬¼ë¦¬, í™”í•™, ìƒë¬¼, ì§€êµ¬ê³¼í•™ ë“± ê³¼í•™ ê´€ë ¨ ë‚´ìš©ì´ ì¶œì œë©ë‹ˆë‹¤.
- **ê¸°ìˆ  ì§€ë¬¸**: ì»´í“¨í„°, ê¸°ê³„, ì‹¤ìƒí™œ ê³¼í•™ ë“±ì´ ì¶œì œë©ë‹ˆë‹¤.
- **ì˜ˆìˆ  ì§€ë¬¸**: ë¯¸ìˆ , ìŒì•…, ê±´ì¶•, ì˜í™” ë“± ë‹¤ì–‘í•œ ì˜ˆìˆ  ì£¼ì œê°€ í¬í•¨ë©ë‹ˆë‹¤.
- **ìœµí•© ì§€ë¬¸**: ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ê²°í•©í•œ ë³µí•©ì ì¸ ì¥ë¬¸ ì§€ë¬¸ë„ ìì£¼ ì¶œì œë©ë‹ˆë‹¤.

- **í‰ê°€ ë‚´ìš©**: ë‹¤ì–‘í•œ ë„ë©”ì¸ì˜ ì´í•´ë ¥ í‰ê°€, ì¶”ë¡  ë° ë¹„íŒì  ì‚¬ê³  ëŠ¥ë ¥ í‰ê°€

### (2) ğŸ§‘â€ğŸ¤ ë¬¸í•™

- **ì¶œì œ ë‚´ìš©**: ê³ ì „ ì†Œì„¤, í˜„ëŒ€ ì†Œì„¤, ê³ ì „ ì‹œê°€, í˜„ëŒ€ ì‹œ, ê³ ì „ìˆ˜í•„ ë“± ë‹¤ì–‘í•œ ë¬¸í•™ ê°ˆë˜ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.

- **í‰ê°€ ë‚´ìš©**: ì •ì„œ ë° ë¬¸ì²´ ì´í•´ë ¥ í‰ê°€, ë‹¤ì–‘í•œ ì‹œëŒ€ì™€ ì¥ë¥´ ì´í•´ í‰ê°€

### (3) ğŸ—£ï¸ í™”ë²•ê³¼ ì‘ë¬¸

- ëŒ€í™”ì™€ ê¸€ì“°ê¸°ë¥¼ ë‹¤ë£¬ ë¬¸ì œ ë“±ì´ ì¶œì œë¨.

- **í‰ê°€ ë‚´ìš©**: ëŒ€í™” ë§¥ë½ ì´í•´ë ¥ í‰ê°€, ì‘ë¬¸ ëŠ¥ë ¥ í‰ê°€

## â™¾ï¸ Metric

### í‰ê°€ ë°©ì‹

- ëŒ€íšŒì—ì„œëŠ” ê° ëª¨ë¸ì´ ì œì‹œëœ ë¬¸ì œì— ëŒ€í•´ ì œì¶œí•œ ë‹µì•ˆì´ ì‹¤ì œ ì •ë‹µê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ ì—¬ë¶€ë¥¼ ì¸¡ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
- í‰ê°€ ì ìˆ˜ëŠ” ê° ë…„ë„ì˜ ë¬¸ì œë³„ë¡œ ì±„ì ë˜ë©°, ìµœì¢…ì ìœ¼ë¡œëŠ” í‘œì¤€ì ìˆ˜ì˜ í‰ê· ì„ í†µí•´ ìˆœìœ„ê°€ ë§¤ê²¨ì§‘ë‹ˆë‹¤.

### ë¦¬ë”ë³´ë“œ ì ìˆ˜ ì„¤ëª…

- **ì›ì ìˆ˜ë€?**: ì‹œí—˜ì—ì„œ 100ì ë§Œì ìœ¼ë¡œ ë°›ì€ ì ìˆ˜
- **í‘œì¤€ì ìˆ˜**: ì‘ì‹œìƒì´ ë°›ì€ ì›ì ìˆ˜ê°€ í‰ê· ì—ì„œ ì–¼ë§ˆë‚˜ ë–¨ì–´ì ¸ ìˆëŠ”ì§€ ì¼ì¢…ì˜ 'í‰ê· ê³¼ì˜ ê±°ë¦¬'ë¥¼ ì¸¡ì •í•˜ëŠ” ì ìˆ˜
- **ë“±ê¸‰**: í‘œì¤€ì ìˆ˜ì— ê·¼ê±°í•´ ìˆ˜í—˜ìƒì„ ë‚˜ëˆˆ ê²ƒìœ¼ë¡œ, ì´ 9ë“±ê¸‰ì´ ìˆë‹¤. êµ­ì–´ì™€ ìˆ˜í•™, íƒêµ¬ì˜ì—­ì—ì„œëŠ” ì˜ì—­ê³¼ëª©ë³„ ì „ì²´ ìˆ˜í—˜ìƒì˜ ìƒìœ„ 4%ê°€ 1ë“±ê¸‰, ê·¸ë‹¤ìŒ 7%(ëˆ„ì  11%)ê¹Œì§€ê°€ 2ë“±ê¸‰, ê·¸ë‹¤ìŒ 12%(ëˆ„ì 
  23%)ê¹Œì§€ê°€ 3ë“±ê¸‰ì´ ëœë‹¤.
  [EBSI ì°¸ê³ ](https://www.ebsi.co.kr/ebs/ent/enta/retrieveEntNewsView.ebs?bbsCd=B011&datNo=142017)

## ğŸ“— ì°¸ê³ í•´ë³¼ë§Œí•œ Reference

- [Nomadamas ì‹¤í—˜ê¸°ë¡](https://github.com/NomaDamas/KICE_slayer_AI_Korean?tab=readme-ov-file#5-%ED%98%95%EC%8B%9D-%EC%A7%80%EC%A0%95-%ED%94%84%EB%A1%AC%ED%94%84%ED%8A%B8)

## ğŸ“° Notice

- ì €ì‘ê¶Œ ë¬¸ì œê°€ ìˆì„ìˆ˜ ìˆì–´ ìˆ˜ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì€ ê³µê°œí•˜ì§€ ì•Šì„ ì˜ˆì •ì…ë‹ˆë‹¤. í‰ê°€ ë°ì´í„°ëŠ” 15ìˆ˜ëŠ¥ ~ 24ìˆ˜ëŠ¥ì´ë©° 22ë…„ë„~24ë…„ë„ ì„ íƒœê³¼ëª©ì€ í™”ë²•ê³¼ ì‘ë¬¸ì— ëŒ€í•´ì„œë§Œ í‰ê°€í•©ë‹ˆë‹¤.
- í‰ê°€ì˜ ê³µì •ì„±ì„ ìœ„í•´ì„œ í”„ë¡¬í”„íŠ¸ëŠ” ê³µê°œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!
- ì¶”í›„ ìˆ˜ëŠ¥ ë‹¹ì¼ ì œì¶œí•´ì£¼ì‹  ëª¨ë¸ë“¤ì„ ì „ë¶€ ë°˜ì˜í•  êµ­ì˜ìˆ˜ì‚¬ê³¼ ëª¨ë‘ ë¦¬ë”ë³´ë“œì—ì„œ ì—…ë°ì´íŠ¸ ì˜ˆì •ì…ë‹ˆë‹¤.
- ë³¸ ìˆ˜ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œì€ [AutoRAG](https://github.com/Marker-Inc-Korea/AutoRAG)ë¥¼ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤!
  (AutoRAG is an automatic RAG optimization tool that can also be used for LLM performance comparison and prompt
  engineering.)

## ğŸ“¬ ë¬¸ì˜í•˜ê¸°

- ê¶ê¸ˆí•œ ì ì´ë‚˜ ì˜¤ë¥˜, ì§€ì›ì´ í•„ìš”í•˜ë‹¤ë©´ ì–¸ì œë“ ì§€ ì—°ë½í•´ ì£¼ì„¸ìš”:

- ì´ë©”ì¼: developerminsing@gmail.com

**ë‹¤ìŒ KO-SAT Slayer Champion**ì´ ë  ì¤€ë¹„ê°€ ë˜ì…¨ë‚˜ìš”? ğŸ’ª


## License
- ë³¸ ë°ì´í„°ì…‹ì˜ ì¶œì²˜ëŠ” [í•œêµ­êµìœ¡ê³¼ì •í‰ê°€ì› (KICE)](https://www.kice.re.kr/main.do?s=kice)ì— ìˆìŠµë‹ˆë‹¤.


-------
This benchmark leaderboard is a non-profit project that aims to provide information on LLM performance with SAT
benchmarks! 
